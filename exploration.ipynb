{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'Loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomlbenchmark/resources/frameworks.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'Loader'"
     ]
    }
   ],
   "source": [
    "with open(\"automlbenchmark/resources/frameworks.yaml\", \"r\") as fp:\n",
    "    yaml.lo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "from src.utils import OpenMLTaskHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = openml.study.get_suite(293)\n",
    "tasks = suite.tasks\n",
    "fn = OpenMLTaskHandler().get_dataset_id_from_task_id\n",
    "datasets = [fn(task_id) for task_id in tasks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing run uploads with missing folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting folds to None\n",
    "- This works and the run is uploaded and metrics computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenML Run\n",
       "==========\n",
       "Uploader Name...................: None\n",
       "Metric..........................: None\n",
       "Local Result - Accuracy (+- STD): 0.9667 +- 0.0333\n",
       "Local Runtime - ms (+- STD).....: 1.7349 +- 0.3808\n",
       "Run ID..........................: 10595211\n",
       "Run URL.........................: https://www.openml.org/r/10595211\n",
       "Task ID.........................: 59\n",
       "Task Type.......................: None\n",
       "Task URL........................: https://www.openml.org/t/59\n",
       "Flow ID.........................: 25115\n",
       "Flow Name.......................: sklearn.neighbors._classification.KNeighborsClassifier\n",
       "Flow URL........................: https://www.openml.org/f/25115\n",
       "Setup ID........................: None\n",
       "Setup String....................: Python_3.9.19. Sklearn_1.5.2. NumPy_1.26.4. SciPy_1.13.1.\n",
       "Dataset ID......................: 61\n",
       "Dataset URL.....................: https://www.openml.org/d/61"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openml\n",
    "from sklearn import neighbors\n",
    "\n",
    "task = openml.tasks.get_task(59)\n",
    "data = openml.datasets.get_dataset(task.dataset_id)\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "run = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False)\n",
    "for fold in range(3):\n",
    "    run.predictions.loc[run.predictions[\"fold\"] == fold, \"predictions\"] = None\n",
    "    run.predictions.loc[run.predictions[\"fold\"] == fold, \"correct\"] = None\n",
    "    run.predictions.loc[run.predictions[\"fold\"] == fold, \"confidence.iris-setosa\"] = None\n",
    "\n",
    "# this key is deprecated\n",
    "openml.config.apikey = \"c9861927df7cfa9baf737189b6eb9976\"\n",
    "run.publish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the folds\n",
    "- This seems to work as well and evaluation metrics are computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "from sklearn import neighbors\n",
    "\n",
    "task = openml.tasks.get_task(59)\n",
    "data = openml.datasets.get_dataset(task.dataset_id)\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "run = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False)\n",
    "for fold in range(3):\n",
    "    run.predictions.drop(run.predictions[run.predictions[\"fold\"] == fold].index, inplace=True)\n",
    "# this key is deprecated\n",
    "openml.config.apikey = \"c9861927df7cfa9baf737189b6eb9976\"\n",
    "run.publish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "from sklearn import neighbors\n",
    "\n",
    "task = openml.tasks.get_task(59)\n",
    "# task.estimation_procedure = {\n",
    "#     \"type\": \"crossvalidation\",\n",
    "#     \"parameters\": {\n",
    "#         \"number_repeats\": \"1\",\n",
    "#         \"number_folds\": \"3\",\n",
    "#         \"percentage\": \"\",\n",
    "#         \"stratified_sampling\": \"true\",\n",
    "#     },\n",
    "#     \"data_splits_url\": \"https://api.openml.org/api_splits/get/125923/Task_125923_splits.arff\",\n",
    "# }\n",
    "data = openml.datasets.get_dataset(task.dataset_id)\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "run = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False)\n",
    "\n",
    "for fold in range(3):\n",
    "    run.predictions.loc[run.predictions[\"fold\"] == fold, \"predictions\"] = None\n",
    "    run.predictions.loc[run.predictions[\"fold\"] == fold, \"correct\"] = None\n",
    "    run.predictions.loc[run.predictions[\"fold\"] == fold, \"confidence.iris-setosa\"] = None\n",
    "\n",
    "# this key is deprecated\n",
    "openml.config.apikey = \"c9861927df7cfa9baf737189b6eb9976\"\n",
    "run.publish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repeat</th>\n",
       "      <th>fold</th>\n",
       "      <th>sample</th>\n",
       "      <th>row_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>correct</th>\n",
       "      <th>confidence.Iris-setosa</th>\n",
       "      <th>confidence.Iris-versicolor</th>\n",
       "      <th>confidence.Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     repeat  fold  sample  row_id       prediction         correct  \\\n",
       "0         0     0       0      43      Iris-setosa     Iris-setosa   \n",
       "1         0     0       0      14      Iris-setosa     Iris-setosa   \n",
       "2         0     0       0      37      Iris-setosa     Iris-setosa   \n",
       "3         0     0       0      23      Iris-setosa     Iris-setosa   \n",
       "4         0     0       0      10      Iris-setosa     Iris-setosa   \n",
       "..      ...   ...     ...     ...              ...             ...   \n",
       "145       0     9       0     142   Iris-virginica  Iris-virginica   \n",
       "146       0     9       0     123  Iris-versicolor  Iris-virginica   \n",
       "147       0     9       0     131   Iris-virginica  Iris-virginica   \n",
       "148       0     9       0     147   Iris-virginica  Iris-virginica   \n",
       "149       0     9       0     146   Iris-virginica  Iris-virginica   \n",
       "\n",
       "     confidence.Iris-setosa  confidence.Iris-versicolor  \\\n",
       "0                       1.0                         0.0   \n",
       "1                       1.0                         0.0   \n",
       "2                       1.0                         0.0   \n",
       "3                       1.0                         0.0   \n",
       "4                       1.0                         0.0   \n",
       "..                      ...                         ...   \n",
       "145                     0.0                         0.2   \n",
       "146                     0.0                         0.5   \n",
       "147                     0.0                         0.0   \n",
       "148                     0.0                         0.1   \n",
       "149                     0.0                         0.2   \n",
       "\n",
       "     confidence.Iris-virginica  \n",
       "0                          0.0  \n",
       "1                          0.0  \n",
       "2                          0.0  \n",
       "3                          0.0  \n",
       "4                          0.0  \n",
       "..                         ...  \n",
       "145                        0.8  \n",
       "146                        0.5  \n",
       "147                        1.0  \n",
       "148                        0.9  \n",
       "149                        0.8  \n",
       "\n",
       "[150 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold == 0, set predictions to NaN\n",
    "for fold in range(3):\n",
    "    run.predictions.loc[run.predictions[\"fold\"] == fold, \"predictions\"] = None\n",
    "    run.predictions.loc[run.predictions[\"fold\"] == fold, \"correct\"] = None\n",
    "    run.predictions.loc[run.predictions[\"fold\"] == fold, \"confidence.iris-setosa\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_file_to_run(run, file, name):\n",
    "    run._old_get_file_elements = run._get_file_elements\n",
    "    \n",
    "    def modified_get_file_elements():\n",
    "        elements = run._old_get_file_elements()\n",
    "        elements[name] = (name, file)\n",
    "        return elements\n",
    "    \n",
    "    run._get_file_elements = modified_get_file_elements\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_add = \"data/generated_reports/report_2.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openml.config.apikey = \"c9861927df7cfa9baf737189b6eb9976\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the file to the run\n",
    "with open(file_to_add, \"rb\") as f:\n",
    "    run = add_file_to_run(run, f.read(), \"report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenML Run\n",
       "==========\n",
       "Uploader Name...................: None\n",
       "Metric..........................: None\n",
       "Local Result - Accuracy (+- STD): 0.9667 +- 0.0333\n",
       "Local Runtime - ms (+- STD).....: 1.4581 +- 0.2746\n",
       "Run ID..........................: 10595203\n",
       "Run URL.........................: https://www.openml.org/r/10595203\n",
       "Task ID.........................: 59\n",
       "Task Type.......................: None\n",
       "Task URL........................: https://www.openml.org/t/59\n",
       "Flow ID.........................: 25126\n",
       "Flow Name.......................: sklearn.neighbors._classification.KNeighborsClassifier\n",
       "Flow URL........................: https://www.openml.org/f/25126\n",
       "Setup ID........................: None\n",
       "Setup String....................: Python_3.10.14. Sklearn_1.6.0. NumPy_2.2.0. SciPy_1.14.1.\n",
       "Dataset ID......................: 61\n",
       "Dataset URL.....................: https://www.openml.org/d/61"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.publish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran = openml.runs.get_run(10595198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ran' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mran\u001b[49m\u001b[38;5;241m.\u001b[39m_get_file_elements()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ran' is not defined"
     ]
    }
   ],
   "source": [
    "ran._get_file_elements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from interpret import set_visualize_provider\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret.provider import InlineProvider\n",
    "from itables import to_html_datatable\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from ollama import ChatResponse\n",
    "from ollama import chat\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional, Union\n",
    "import json\n",
    "import markdown\n",
    "import matplotlib\n",
    "import openml\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from typing import Any\n",
    "from src.utils import OpenMLTaskHandler\n",
    "\n",
    "matplotlib.use(\"agg\")\n",
    "set_visualize_provider(InlineProvider())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_load_file(file_path, file_type) -> Union[pd.DataFrame, dict, None]:\n",
    "    \"\"\"\n",
    "    This function is responsible for safely loading a file. It returns None if the file is not found or if there is an error loading the file.\n",
    "    \"\"\"\n",
    "    if file_type == \"json\":\n",
    "        try:\n",
    "            with open(str(Path(file_path)), \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return None\n",
    "    elif file_type == \"pd\":\n",
    "        try:\n",
    "            return pd.read_csv(str(file_path))\n",
    "        except:\n",
    "            return None\n",
    "    elif file_type == \"textdict\":\n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                return json.loads(f.read())\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResultCollector:\n",
    "    def __init__(self, path: str = \"../data/results/*\"):\n",
    "        self.experiment_directory = Path(path)\n",
    "        self.all_run_paths = glob(pathname=str(self.experiment_directory))\n",
    "        self.all_results = pd.DataFrame()\n",
    "        self.openml_task_handler = OpenMLTaskHandler()\n",
    "        # Required columns\n",
    "        self.required_columns = {\n",
    "            \"metric\",\n",
    "            \"result\",\n",
    "            \"framework\",\n",
    "            \"dataset_id\",\n",
    "            \"id\",\n",
    "            \"task\",\n",
    "            \"predict_duration\",\n",
    "            \"models\",\n",
    "        }\n",
    "\n",
    "        # Define how to find the best result for the metric\n",
    "        self.metric_used_dict = {\n",
    "            \"auc\": lambda x: x.max(),\n",
    "            \"neg_logloss\": lambda x: x.min(),\n",
    "        }\n",
    "\n",
    "    def get_dataset_description_from_id(self, dataset_id: int) -> Optional[str]:\n",
    "        dataset_id = int(dataset_id)\n",
    "        return openml.datasets.get_dataset(dataset_id).description\n",
    "\n",
    "    def collect_all_run_info_to_df(self):\n",
    "        \"\"\"\n",
    "        This function is responsible for loading all the results files from the runs and storing them in self.all_results. This is further used to generate the dashboard.\n",
    "        \"\"\"\n",
    "        all_results_list = []  # Temporary list to store individual DataFrames\n",
    "\n",
    "        for run_path in tqdm(self.all_run_paths, total=len(self.all_run_paths)):\n",
    "            run_path = Path(run_path)\n",
    "            results_file_path = run_path / \"results.csv\"\n",
    "\n",
    "            # Load results file if it exists\n",
    "            results_file = safe_load_file(results_file_path, \"pd\")\n",
    "\n",
    "            # If results file is loaded, proceed to process it\n",
    "            if results_file is not None:\n",
    "                # Get the model path specific to this run_path\n",
    "                models_path_list = list((run_path / \"models\").rglob(\"models.*\"))\n",
    "                leaderboard_path_list = list(\n",
    "                    (run_path / \"models\").rglob(\"leaderboard.*\")\n",
    "                )\n",
    "                # models_path = str(models_path_list[0]) if len(models_path_list) >0 else None\n",
    "\n",
    "                if len(models_path_list) > 0:\n",
    "                    models_path = str(models_path_list[0])\n",
    "                elif len(leaderboard_path_list) > 0:\n",
    "                    models_path = str(leaderboard_path_list[0])\n",
    "                else:\n",
    "                    models_path = None\n",
    "\n",
    "                # Add the model path as a new column in the current results_file DataFrame\n",
    "                results_file[\"models\"] = models_path\n",
    "\n",
    "                # Get the dataset ID for each row in the results file\n",
    "                try:\n",
    "                    results_file[\"dataset_id\"] = results_file[\"id\"].apply(\n",
    "                        self.openml_task_handler.get_dataset_id_from_task_id\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    results_file[\"dataset_id\"] = None\n",
    "\n",
    "                results_file[\"dataset_description\"] = results_file[\"dataset_id\"].apply(\n",
    "                    self.get_dataset_description_from_id\n",
    "                )\n",
    "\n",
    "                # Append the processed DataFrame to our list\n",
    "                all_results_list.append(results_file)\n",
    "\n",
    "        # Concatenate all individual DataFrames into self.all_results\n",
    "        if all_results_list:\n",
    "            self.all_results = pd.concat(all_results_list, ignore_index=True)\n",
    "\n",
    "    def validate_dataframe_and_add_extra_info(self):\n",
    "        # Validate DataFrame\n",
    "        if self.all_results is None or self.all_results.empty:\n",
    "            return \"Error: Provided DataFrame is empty or None.\"\n",
    "\n",
    "        # Handle duplicate frameworks by keeping the one with the best result\n",
    "        self.all_results = self.all_results.drop_duplicates(\n",
    "            subset=[\"framework\"], keep=\"first\"\n",
    "        )\n",
    "\n",
    "        # Add missing columns with default values\n",
    "        for column in self.required_columns:\n",
    "            if column not in self.all_results.columns:\n",
    "                self.all_results[column] = \"N/A\"\n",
    "\n",
    "    def __call__(self):\n",
    "        self.collect_all_run_info_to_df()\n",
    "        return self.all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureImportance:\n",
    "    \"\"\"\n",
    "    Feature Importance using Explainable Boosting Classifier from InterpretML\n",
    "    \"\"\"\n",
    "    def generate_ebm_report(self, names, scores):\n",
    "        df = pd.DataFrame({\"Feature\": names, \"Score\": scores}).sort_values(by=\"Score\", ascending=False)\n",
    "        fig = px.bar(df, x=\"Score\", y=\"Feature\", orientation=\"h\")\n",
    "        fig.update_layout(\n",
    "            title=\"Feature Importance\",\n",
    "            xaxis_title=\"Score\",\n",
    "            yaxis_title=\"Feature\",\n",
    "            width=800,\n",
    "            height=800,\n",
    "        )\n",
    "        fig_html= fig.to_html(full_html=False, include_plotlyjs=\"cdn\")\n",
    "        return f\"\"\"<div id=\"feature-importance\" class=\"container\" style=\"margin-top: 20px; text-align: left\">{fig_html}</div>\"\"\"\n",
    "\n",
    "    def run_ebm_on_dataset(self, dataset_id, X_train, y_train):\n",
    "        try:\n",
    "            ebm = ExplainableBoostingClassifier(random_state=42)\n",
    "            ebm.fit(X_train, y_train)\n",
    "            ebm_global = ebm.explain_global().data()\n",
    "            names, scores = ebm_global[\"names\"], ebm_global[\"scores\"]\n",
    "            return self.generate_ebm_report(names, scores)\n",
    "        except Exception as e:\n",
    "            print(f\"Error running EBM on dataset {dataset_id}: {e}\")\n",
    "            return \"<div>Unable to generate feature importance report</div>\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataOverviewGenerator:\n",
    "    \"\"\"\n",
    "    This class does the following\n",
    "    - Generates a data summary table for the dataset\n",
    "    - Generates the Feature Importance report using Explainable Boosting Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, template_dir=\"./website_assets/templates/\"\n",
    "    ):\n",
    "        self.template_dir = template_dir\n",
    "        self.jinja_environment = Environment(loader=FileSystemLoader(template_dir))\n",
    "        self.template_to_use = {\"data_summary_table\": \"data_summary_table.html\"}\n",
    "        self.explainable_boosting = FeatureImportance()\n",
    "\n",
    "    def get_data_and_split(self, dataset_id):\n",
    "        dataset = openml.datasets.get_dataset(dataset_id=dataset_id)\n",
    "        X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
    "        X = pd.get_dummies(X, prefix_sep=\".\").astype(float)\n",
    "        y, _ = y.factorize()\n",
    "        return train_test_split(X, y, random_state=42)\n",
    "\n",
    "    def generate_data_summary_table(self, X, max_cols=100):\n",
    "        col_visualizations = {}\n",
    "        # Get the first 100 columns if max_cols is not specified. This is to avoid generating too many visualizations.\n",
    "        if max_cols is not None:\n",
    "            cols = X.columns[:max_cols]\n",
    "\n",
    "        # Generate visualizations for each column. Pie or histogram based on the number of unique values.\n",
    "        self.generate_visualizations_for_each_column(X, col_visualizations, cols)\n",
    "\n",
    "        try:\n",
    "            visualization_row = pd.DataFrame([col_visualizations])\n",
    "            missing_values = X.isnull().sum().to_frame().T\n",
    "            missing_values.index = [\"Missing Values\"]\n",
    "            X_preview = X.head(10)\n",
    "\n",
    "            table_data = pd.concat(\n",
    "                [visualization_row, missing_values, X_preview], ignore_index=True\n",
    "            )\n",
    "            row_names = [\"Visualization\", \"Missing Values\"]\n",
    "            # Assign the 'Headers' column directly\n",
    "            table_data[\"Headers\"] = row_names + [\"\"] * (\n",
    "                len(table_data) - len(row_names)\n",
    "            )\n",
    "\n",
    "            # Rearrange columns to move 'Headers' to the front\n",
    "            table_data = table_data[\n",
    "                [\"Headers\"] + [col for col in table_data.columns if col != \"Headers\"]\n",
    "            ]\n",
    "\n",
    "            headers_html = \"\".join(f\"<th>{col}</th>\" for col in table_data.columns)\n",
    "            rows_html = \"\".join(\n",
    "                \"<tr>\" + \"\".join(f\"<td>{cell}</td>\" for cell in row) + \"</tr>\"\n",
    "                for row in table_data.values\n",
    "            )\n",
    "\n",
    "            return (\n",
    "                f\"\"\"\n",
    "        <div class=\"container\" style=\"overflow-x:auto;\">\n",
    "            <h2>Data Summary Table</h2>\n",
    "            <input type=\"text\" id=\"searchBar\" placeholder=\"Search columns...\" style=\"margin-bottom:10px; width:100%;\">\n",
    "            <table id=\"dataTable\" class=\"table table-striped table-bordered\" style=\"display:none;\">\n",
    "                <thead><tr>{headers_html}</tr></thead>\n",
    "                <tbody>{rows_html}</tbody>\n",
    "            </table>\n",
    "            <button id=\"prevBtn\">Previous</button>\n",
    "            <button id=\"nextBtn\">Next</button>\n",
    "        </div>\"\"\"\n",
    "                + \"\"\"\n",
    "        <script>\n",
    "            const table = document.getElementById('dataTable');\n",
    "            const searchBar = document.getElementById('searchBar');\n",
    "            const cols = table.querySelectorAll('thead th');\n",
    "            const rows = table.querySelectorAll('tbody tr');\n",
    "            const totalCols = cols.length;\n",
    "            const colsPerPage = 10;\n",
    "            let startCol = 0;\n",
    "\n",
    "            function updateTable() {\n",
    "                cols.forEach((col, i) => col.style.display = i >= startCol && i < startCol + colsPerPage ? '' : 'none');\n",
    "                rows.forEach(row => {\n",
    "                    Array.from(row.children).forEach((cell, i) => cell.style.display = i >= startCol && i < startCol + colsPerPage ? '' : 'none');\n",
    "                });\n",
    "                document.getElementById('prevBtn').disabled = startCol === 0;\n",
    "                document.getElementById('nextBtn').disabled = startCol + colsPerPage >= totalCols;\n",
    "            }\n",
    "\n",
    "            searchBar.addEventListener('input', () => {\n",
    "                const query = searchBar.value.toLowerCase();\n",
    "                cols.forEach((col, i) => {\n",
    "                    if (col.textContent.toLowerCase().includes(query)) {\n",
    "                        col.style.display = '';\n",
    "                        rows.forEach(row => {\n",
    "                            row.children[i].style.display = '';\n",
    "                        });\n",
    "                    } else {\n",
    "                        col.style.display = 'none';\n",
    "                        rows.forEach(row => {\n",
    "                            row.children[i].style.display = 'none';\n",
    "                        });\n",
    "                    }\n",
    "                });\n",
    "            });\n",
    "\n",
    "            document.getElementById('prevBtn').addEventListener('click', () => {\n",
    "                if (startCol > 0) startCol -= colsPerPage;\n",
    "                updateTable();\n",
    "            });\n",
    "\n",
    "            document.getElementById('nextBtn').addEventListener('click', () => {\n",
    "                if (startCol + colsPerPage < totalCols) startCol += colsPerPage;\n",
    "                updateTable();\n",
    "            });\n",
    "\n",
    "            table.style.display = '';\n",
    "            updateTable();\n",
    "        </script>\n",
    "        \"\"\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating data summary table: {e}\")\n",
    "            return \"<div>Unable to generate data summary table</div>\"\n",
    "\n",
    "    def generate_visualizations_for_each_column(self, X, col_visualizations, cols):\n",
    "        for col in cols:\n",
    "            try:\n",
    "                X[col] = X[col].astype(str)\n",
    "                unique_values = len(X[col].unique())\n",
    "\n",
    "                if unique_values < 10:\n",
    "                    fig = px.histogram(X, x=col)\n",
    "                else:\n",
    "                    value_counts = X[col].value_counts()\n",
    "                    if len(value_counts) > 2:\n",
    "                        value_counts = pd.concat(\n",
    "                            [\n",
    "                                value_counts[:2],\n",
    "                                pd.Series([value_counts[2:].sum()], index=[\"Other\"]),\n",
    "                            ]\n",
    "                        )\n",
    "                    fig = px.pie(names=value_counts.index, values=value_counts.values)\n",
    "\n",
    "                fig.update_layout(\n",
    "                    title=None,\n",
    "                    width=300,\n",
    "                    height=300,\n",
    "                )\n",
    "                col_visualizations[col] = fig.to_html(\n",
    "                    full_html=False, include_plotlyjs=\"cdn\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing column '{col}': {e}\")\n",
    "                col_visualizations[col] = \"\"\n",
    "\n",
    "    \n",
    "    def generate_complete_report(self, dataset_id):\n",
    "        try:\n",
    "            X_train, _, y_train, _ = self.get_data_and_split(dataset_id)\n",
    "            ebm_report = self.explainable_boosting.run_ebm_on_dataset(\n",
    "                dataset_id, X_train, y_train\n",
    "            )\n",
    "            data_summary_table = self.generate_data_summary_table(X_train)\n",
    "            return data_summary_table, ebm_report\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating report for dataset {dataset_id}: {e}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsFromAMLB:\n",
    "    \"\"\"This class is responsible for storing metrics and their related information. To add a new metric, add it here.\n",
    "    !Important note: This was generated by ChatGPT and may not be accurate. Please feel free to modify it as needed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.metrics_info = {\n",
    "            \"binary_metrics\": {\n",
    "                \"auc\": {\n",
    "                    \"description\": \"Area Under the ROC Curve, measures classification performance.\",\n",
    "                    \"better_value\": \"higher\",\n",
    "                },\n",
    "                \"logloss\": {\n",
    "                    \"description\": \"Logarithmic Loss, evaluates how well probabilities are predicted.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"acc\": {\n",
    "                    \"description\": \"Accuracy, proportion of correctly classified instances.\",\n",
    "                    \"better_value\": \"higher\",\n",
    "                },\n",
    "                \"balacc\": {\n",
    "                    \"description\": \"Balanced Accuracy, accounts for class imbalance.\",\n",
    "                    \"better_value\": \"higher\",\n",
    "                },\n",
    "            },\n",
    "            \"multiclass_metrics\": {\n",
    "                \"logloss\": {\n",
    "                    \"description\": \"Logarithmic Loss, evaluates how well probabilities are predicted.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"acc\": {\n",
    "                    \"description\": \"Accuracy, proportion of correctly classified instances.\",\n",
    "                    \"better_value\": \"higher\",\n",
    "                },\n",
    "                \"balacc\": {\n",
    "                    \"description\": \"Balanced Accuracy, accounts for class imbalance.\",\n",
    "                    \"better_value\": \"higher\",\n",
    "                },\n",
    "            },\n",
    "            \"regression_metrics\": {\n",
    "                \"rmse\": {\n",
    "                    \"description\": \"Root Mean Square Error, measures prediction error magnitude.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"r2\": {\n",
    "                    \"description\": \"R-squared, proportion of variance explained by the model.\",\n",
    "                    \"better_value\": \"higher\",\n",
    "                },\n",
    "                \"mae\": {\n",
    "                    \"description\": \"Mean Absolute Error, average of absolute prediction errors.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "            },\n",
    "            \"timeseries_metrics\": {\n",
    "                \"mase\": {\n",
    "                    \"description\": \"Mean Absolute Scaled Error, compares model error to naive forecasting error.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"mape\": {\n",
    "                    \"description\": \"Mean Absolute Percentage Error, measures percentage prediction error.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"smape\": {\n",
    "                    \"description\": \"Symmetric Mean Absolute Percentage Error, measures scaled percentage error.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"wape\": {\n",
    "                    \"description\": \"Weighted Absolute Percentage Error, weighted measure of error.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"rmse\": {\n",
    "                    \"description\": \"Root Mean Square Error, measures prediction error magnitude.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"mse\": {\n",
    "                    \"description\": \"Mean Squared Error, average of squared prediction errors.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"mql\": {\n",
    "                    \"description\": \"Mean Quantile Loss, evaluates quantile regression performance.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"wql\": {\n",
    "                    \"description\": \"Weighted Quantile Loss, weighted measure for quantile regression.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "                \"sql\": {\n",
    "                    \"description\": \"Sum Quantile Loss, cumulative quantile regression error.\",\n",
    "                    \"better_value\": \"lower\",\n",
    "                },\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestResult:\n",
    "    \"\"\"This generates the Best result table for the dashboard\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, current_results, metrics_info, jinja_environment, template_to_use\n",
    "    ):\n",
    "        self.current_results = current_results\n",
    "        self.metrics_info = metrics_info\n",
    "        self.jinja_environment = jinja_environment\n",
    "        self.template_to_use = template_to_use\n",
    "        self.best_framework = \"\"\n",
    "        self.best_metric = \"\"\n",
    "        self.type_of_task = \"\"\n",
    "        self.dataset_id = \"\"\n",
    "        self.task_id = \"\"\n",
    "        self.task_name = \"\"\n",
    "        self.best_result_for_metric = \"\"\n",
    "        self.description = \"\"\n",
    "        self.metric_and_result = \"\"\n",
    "\n",
    "    def get_best_result(self):\n",
    "        \"\"\"\n",
    "        This function returns the best result from the current_results DataFrame.\n",
    "        It first sorts the DataFrame based on the metric used and then returns the best result.\n",
    "        \"\"\"\n",
    "        if self.current_results is None:\n",
    "            return None\n",
    "\n",
    "        metric_used = self.current_results[\"metric\"].iloc[0]\n",
    "        sort_in_ascending_order = True  # Default to ascending order\n",
    "\n",
    "        # Determine sorting order based on metrics_info\n",
    "        for category, metrics in self.metrics_info.items():\n",
    "            if metric_used in metrics:\n",
    "                sort_in_ascending_order = (\n",
    "                    metrics[metric_used][\"better_value\"] == \"lower\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "        sorted_results = self.current_results.sort_values(\n",
    "            by=\"result\", ascending=sort_in_ascending_order\n",
    "        ).head()\n",
    "\n",
    "        best_result = sorted_results.iloc[0]\n",
    "        self.best_framework = best_result.get(\"framework\", \"\")\n",
    "        self.best_metric = best_result.get(\"metric\", \"\")\n",
    "        self.type_of_task = best_result.get(\"type\", \"\")\n",
    "        self.dataset_id = best_result.get(\"dataset_id\", \"\")\n",
    "        self.task_id = \"https://\" + best_result.get(\"id\", \"\")\n",
    "        self.task_name = best_result.get(\"task\", \"\")\n",
    "        self.best_result_for_metric = best_result.get(\"result\", \"\")\n",
    "        self.description = best_result.get(\"dataset_description\", \"\")\n",
    "\n",
    "        # all metric columns that are in the dataframe and in the metrics_info\n",
    "        metric_columns = [\n",
    "            col\n",
    "            for col in self.current_results.columns\n",
    "            if any(col in metrics for metrics in self.metrics_info.values())\n",
    "        ]\n",
    "\n",
    "        self.all_metrics_present = []\n",
    "        for metric in metric_columns:\n",
    "            try:\n",
    "                self.all_metrics_present.append(self.current_results[metric].values[0])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        self.metric_and_result = \" \".join(\n",
    "            [\n",
    "                f\"The {metric} is {result} \"\n",
    "                for metric, result in zip(metric_columns, self.all_metrics_present)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def generate_best_result_table(self):\n",
    "        \"\"\"\n",
    "        This function generates the best result table using the best result information.\n",
    "        \"\"\"\n",
    "        template = self.jinja_environment.get_template(\n",
    "            self.template_to_use[\"best_result\"]\n",
    "        )\n",
    "        try:\n",
    "            return template.render(\n",
    "                best_framework=self.best_framework,\n",
    "                best_metric=self.best_metric,\n",
    "                type_of_task=self.type_of_task,\n",
    "                dataset_id=self.dataset_id,\n",
    "                task_id=self.task_id,\n",
    "                task_name=self.task_name,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating best result table: {e}\")\n",
    "            return \"<div>Unable to generate best result table</div>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMExplanation:\n",
    "    def __init__(self, best_result: BestResult, model=\"llama3.2\", temperature=0.3):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.best_result = best_result\n",
    "\n",
    "    def get_explanation_from_llm(self):\n",
    "        \"\"\"\n",
    "        Based on information obtained from the AutoML systems and OpenML generate an explanation using LLM.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_format = f\"\"\"For a dataset called {self.best_result.task_name} , the best framework is {self.best_result.best_framework} with a {self.best_result.best_metric} of {self.best_result.best_result_for_metric}. This is a {self.best_result.type_of_task} task. The results are as follows {self.best_result.metric_and_result}. For each metric, tell me if this is a good score (and why), and if it is not, how can I improve it? Keep your answer to the point.\n",
    "        The dataset description is: {self.best_result.description}\n",
    "        \"\"\"\n",
    "        response: ChatResponse = chat(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt_format,\n",
    "                },\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": self.temperature,\n",
    "            },\n",
    "        )\n",
    "        response = response[\"message\"][\"content\"]\n",
    "        response = markdown.markdown(response)\n",
    "\n",
    "        return f\"\"\"<div\n",
    "            style=\"text-align: left; margin-bottom: 20px\"\n",
    "            >\n",
    "            <h1>Explanation and What's next?</h1>\n",
    "            <p>\n",
    "                !!! This is an AI-generated (llama3.2) explanation of the results.\n",
    "                Please take the response with a grain of salt and use your own\n",
    "                judgement.\n",
    "            </p>\n",
    "            {response}\n",
    "            </div>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate complete report for ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FrameworkProcessor:\n",
    "    # def __init__(self, framework_name, process_fn):\n",
    "    framework_name: str\n",
    "    process_fn: Any\n",
    "\n",
    "\n",
    "class GenerateCompleteReportForDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_id: int,\n",
    "        collector_results,\n",
    "        GENERATED_REPORTS_DIR: str = \"../data/generated_reports\",\n",
    "        GENERATED_DATA_REPORT_DIR: str = \"../data/generated_data_reports\",\n",
    "        template_dir=\"./website_assets/templates/\",\n",
    "    ):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.collector_results = collector_results\n",
    "        self.current_results = self.get_results_for_dataset_id(self.dataset_id)\n",
    "        self.jinja_environment = Environment(loader=FileSystemLoader(template_dir))\n",
    "        self.generated_final_reports_dir = GENERATED_REPORTS_DIR\n",
    "        self.generated_data_reports_dir = GENERATED_DATA_REPORT_DIR\n",
    "        self.template_dir = template_dir\n",
    "        self.template_to_use = {\n",
    "            \"dataset_info\": \"data_information.html\",\n",
    "            \"best_result\": \"best_result_table.html\",\n",
    "            \"framework_table\": \"framework_table.html\",\n",
    "            \"metric_vs_result\": \"metric_vs_result.html\",\n",
    "        }\n",
    "        self.framework_processor = FrameworkProcessor\n",
    "        # all metrics that are in the dataframe\n",
    "        self.metrics_info = MetricsFromAMLB().metrics_info\n",
    "\n",
    "        # run the function to get the best result\n",
    "        self.frameworks_that_support_extra_information = [\n",
    "            \"Auto-sklearn\",\n",
    "            \"H20AutoML\",\n",
    "            \"AutoGluon\",\n",
    "            \"All results\",\n",
    "        ]\n",
    "\n",
    "        self.framework_processors = [\n",
    "            FrameworkProcessor(\"Auto-sklearn\", self.process_auto_sklearn_data),\n",
    "            FrameworkProcessor(\"H20AutoML\", self.process_h2oautoml_data),\n",
    "            FrameworkProcessor(\"AutoGluon\", self.process_auto_gluon_data),\n",
    "            FrameworkProcessor(\"All results\", self.process_all_results_data),\n",
    "        ]\n",
    "\n",
    "        self.best_result = BestResult(\n",
    "            self.current_results,\n",
    "            self.metrics_info,\n",
    "            self.jinja_environment,\n",
    "            self.template_to_use,\n",
    "        )\n",
    "        # for all init in best result, add them to the current object\n",
    "        self.best_result.get_best_result()\n",
    "\n",
    "    def get_results_for_dataset_id(self, dataset_id: int) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        This function returns the results for a given dataset_id. If no results are found, it returns None.\n",
    "        \"\"\"\n",
    "        results_for_dataset = self.collector_results[\n",
    "            self.collector_results[\"dataset_id\"] == dataset_id\n",
    "        ]\n",
    "        if results_for_dataset.empty:\n",
    "            return None\n",
    "        return results_for_dataset\n",
    "\n",
    "    def generate_dataset_info(self):\n",
    "        \"\"\"\n",
    "        This function generates the dataset information table using the dataset information.\n",
    "        \"\"\"\n",
    "        template = self.jinja_environment.get_template(\n",
    "            self.template_to_use[\"dataset_info\"]\n",
    "        )\n",
    "        return template.render(\n",
    "            dataset_id=self.dataset_id,\n",
    "            task_name=self.best_result.task_name,\n",
    "        )\n",
    "\n",
    "    def process_h2oautoml_data(self, current_results, top_n=10):\n",
    "        # TODO\n",
    "        return self.get_rows_for_framework_from_df(\n",
    "            df=current_results, framework_name=\"H20AutoML\", top_n=10\n",
    "        )\n",
    "\n",
    "    def process_auto_gluon_data(self, current_results, top_n=10):\n",
    "        # TODO\n",
    "        return self.get_rows_for_framework_from_df(\n",
    "            df=current_results, framework_name=\"AutoGluon\", top_n=10\n",
    "        )\n",
    "\n",
    "    def process_all_results_data(self, df, top_n=40):\n",
    "        try:\n",
    "            df = df.drop(columns=\"dataset_description\", errors=\"ignore\")\n",
    "            return to_html_datatable(\n",
    "                df,\n",
    "                caption=\"Results by AutoML Framework\",\n",
    "                table_id=\"all-framework-results\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return \"\"\n",
    "\n",
    "    def process_auto_sklearn_data(self, df, top_n=10):\n",
    "        auto_sklearn_data = pd.DataFrame()\n",
    "        try:\n",
    "            auto_sklearn_rows = df[df[\"framework\"] == \"autosklearn\"]\n",
    "            # for each row, read the json file from the models column and get the model id and cost\n",
    "            for _, row in auto_sklearn_rows.iterrows():\n",
    "                models_path = row[\"models\"]\n",
    "                try:\n",
    "                    with open(models_path, \"r\") as f:\n",
    "                        models_file = json.load(f)\n",
    "                        for model in models_file:\n",
    "                            auto_sklearn_data = pd.concat(\n",
    "                                [auto_sklearn_data, pd.DataFrame([models_file[model]])],\n",
    "                                ignore_index=True,\n",
    "                            )\n",
    "                except:\n",
    "                    pass\n",
    "                auto_sklearn_data = auto_sklearn_data.sort_values(\n",
    "                    by=\"cost\", ascending=True\n",
    "                ).head(top_n)\n",
    "                return to_html_datatable(\n",
    "                    auto_sklearn_data, caption=\"Auto Sklearn Models\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return \"<div></div>\"\n",
    "\n",
    "    def get_rows_for_framework_from_df(\n",
    "        self, df: pd.DataFrame, framework_name, top_n=40\n",
    "    ):\n",
    "        try:\n",
    "            framework_rows = df[df[\"framework\"] == framework_name][\"models\"].values[0]\n",
    "            framework_data = safe_load_file(framework_rows, \"pd\")\n",
    "            return to_html_datatable(\n",
    "                framework_data.head(top_n), caption=f\"{framework_name} Models\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return \"\"\n",
    "\n",
    "    def generate_framework_table(self):\n",
    "        complete_html = \"\"\n",
    "        for processor in self.framework_processors:\n",
    "            try:\n",
    "                complete_html += processor.process_fn(self.current_results)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "        return f\"\"\"\n",
    "        <div class=\"container\" style=\"margin-top: 20px; text-align: left;\">\n",
    "            {complete_html}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    def generate_dashboard_section(self):\n",
    "        dashboard_html = f\"\"\"\n",
    "        <div style=\"text-align: left; margin: 20px 0;\">\n",
    "            <h1>Framework Performance Dashboard</h1>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 30px; margin-bottom: 40px;\">\n",
    "            {self.graph_and_heading(\n",
    "                self.current_results,\n",
    "                self.best_result.best_metric.upper() + \"-task\",\n",
    "                \"task\",\n",
    "                \"result\",\n",
    "                \"framework\",\n",
    "                f\"{self.best_result.best_metric.upper()} of each Framework\",\n",
    "                \"1\",\n",
    "                \"This is a plot of the main metric used in the experiment against the result of the experiment for each framework for each task. Use this plot to compare the performance of each framework for each task.\",\n",
    "                \"bar\"\n",
    "            )}\n",
    "            {self.graph_and_heading(\n",
    "                self.current_results,\n",
    "                \"predict-duration-task\",\n",
    "                \"framework\",\n",
    "                \"predict_duration\",\n",
    "                \"framework\",\n",
    "                \"Predict Duration of each Framework\",\n",
    "                \"2\",\n",
    "                \"This is a plot of the prediction duration for each framework for each task. Use this plot to find the framework with the fastest prediction time.\",\n",
    "                \"bar\"\n",
    "            )}\n",
    "            {self.graph_and_heading(\n",
    "                self.current_results,\n",
    "                \"framework-performance\",\n",
    "                \"framework\",\n",
    "                \"result\",\n",
    "                \"framework\",\n",
    "                \"Performance of each Framework\",\n",
    "                \"1\",\n",
    "                \"This is a plot of the performance of each framework for each task. Use this plot find the best framework for the tasks.\",\n",
    "                \"bar\"\n",
    "            )}\n",
    "            {self.graph_and_heading(\n",
    "                self.current_results,\n",
    "                \"predict-duration-performance\",\n",
    "                \"predict_duration\",\n",
    "                \"result\",\n",
    "                \"framework\",\n",
    "                \"Predict Duration vs Performance\",\n",
    "                \"2\",\n",
    "                \"This is a scatter plot of the prediction duration against the performance of each framework for each task. Use this plot to find the best framework for the tasks.\",\n",
    "                \"scatter\"\n",
    "            )}\n",
    "        </div>\n",
    "\n",
    "        \"\"\"\n",
    "        return dashboard_html\n",
    "\n",
    "    def graph_and_heading(\n",
    "        self,\n",
    "        df,\n",
    "        graph_id,\n",
    "        x,\n",
    "        y,\n",
    "        color,\n",
    "        title,\n",
    "        grid_column,\n",
    "        description,\n",
    "        plot_type=\"bar\",\n",
    "    ):\n",
    "        try:\n",
    "            colors = px.colors.qualitative.Safe\n",
    "            if len(x) == 0:\n",
    "                return \"<div></div>\"\n",
    "\n",
    "            # use plotly to create the plot\n",
    "            if plot_type == \"bar\":\n",
    "                fig = px.bar(\n",
    "                    df,\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    color=color,\n",
    "                    title=title,\n",
    "                    color_discrete_sequence=colors,\n",
    "                )\n",
    "            elif plot_type == \"scatter\":\n",
    "                fig = px.scatter(\n",
    "                    df,\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    color=color,\n",
    "                    title=title,\n",
    "                    color_discrete_sequence=colors,\n",
    "                )\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=title,\n",
    "                xaxis_title=x,\n",
    "                yaxis_title=y,\n",
    "            )\n",
    "            encoded_image = fig.to_html(full_html=False, include_plotlyjs=\"cdn\")\n",
    "\n",
    "            return f\"<div style='margin-top: 20px; text-align: left grid-column: {grid_column};'>{encoded_image}</div>\"\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return f\"<div style='margin-top: 20px; text-align: left grid-column: {grid_column};'><p>Error generating graph: {str(e)}</p></div>\"\n",
    "\n",
    "    def __call__(self):\n",
    "        report_path = Path(self.generated_final_reports_dir) / f\"report_{self.dataset_id}.html\"\n",
    "        if report_path.exists():\n",
    "            return\n",
    "        data_overview = DataOverviewGenerator(\n",
    "            self.generated_data_reports_dir, self.template_dir\n",
    "        )\n",
    "        data_summary_table, ebm_report = data_overview.generate_complete_report(self.dataset_id)\n",
    "        if data_summary_table is None or ebm_report is None:\n",
    "            return\n",
    "\n",
    "        dataset_info = self.generate_dataset_info()\n",
    "        best_result_table = self.best_result.generate_best_result_table()\n",
    "        framework_table = self.generate_framework_table()\n",
    "        dashboard_section = self.generate_dashboard_section()\n",
    "        explanation = LLMExplanation(\n",
    "            best_result=self.best_result\n",
    "        ).get_explanation_from_llm()\n",
    "\n",
    "        combined_html = self.jinja_environment.get_template(\n",
    "            \"complete_page.html\"\n",
    "        ).render(\n",
    "            dataset_info=dataset_info,\n",
    "            best_result_table=best_result_table,\n",
    "            framework_table=framework_table,\n",
    "            dashboard_section=dashboard_section,\n",
    "            explanation=explanation,\n",
    "            ebm_report=ebm_report,\n",
    "            data_summary_table=data_summary_table,\n",
    "        )\n",
    "\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(combined_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_report_script_for_dataset(\n",
    "    GENERATED_REPORTS_DIR, dataset_id, result_path, template_dir\n",
    "):\n",
    "    # collect all the results from the runs\n",
    "    collector = ResultCollector(result_path)\n",
    "    all_results = collector()\n",
    "    drg = DataOverviewGenerator()\n",
    "    try:\n",
    "        # generate the data report for all datasets\n",
    "        drg.generate_complete_report(dataset_id=dataset_id)\n",
    "        # write complete report to a file\n",
    "        GenerateCompleteReportForDataset(\n",
    "            dataset_id=dataset_id,\n",
    "            collector_results=all_results,\n",
    "            GENERATED_REPORTS_DIR=GENERATED_REPORTS_DIR,\n",
    "            template_dir=template_dir,\n",
    "        )()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report for dataset {dataset_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_REPORTS_DIR = Path(\"./data/generated_reports\")\n",
    "GENERATED_REPORTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eabe8e6f5d341dda894af40802ce3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smukherjee/.pyenv/versions/automlb/lib/python3.9/site-packages/interpret/glassbox/_ebm/_ebm.py:751: UserWarning:\n",
      "\n",
      "Missing values detected. Our visualizations do not currently display missing values. To retain the glassbox nature of the model you need to either set the missing values to an extreme value like -1000 that will be visible on the graphs, or manually examine the missing value score in ebm.term_scores_[term_index][0]\n",
      "\n",
      "/Users/smukherjee/.pyenv/versions/automlb/lib/python3.9/site-packages/interpret/glassbox/_ebm/_ebm.py:1019: UserWarning:\n",
      "\n",
      "Detected multiclass problem. Forcing interactions to 0. Multiclass interactions only have local explanations. They are not currently displayed in the global explanation visualizations. Set interactions=0 to disable this warning. If you still want multiclass interactions, this API accepts a list, and the measure_interactions function can be used to detect them.\n",
      "\n",
      "/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_40364/3816053816.py:44: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "/Users/smukherjee/.pyenv/versions/automlb/lib/python3.9/site-packages/interpret/glassbox/_ebm/_ebm.py:751: UserWarning:\n",
      "\n",
      "Missing values detected. Our visualizations do not currently display missing values. To retain the glassbox nature of the model you need to either set the missing values to an extreme value like -1000 that will be visible on the graphs, or manually examine the missing value score in ebm.term_scores_[term_index][0]\n",
      "\n",
      "/Users/smukherjee/.pyenv/versions/automlb/lib/python3.9/site-packages/interpret/glassbox/_ebm/_ebm.py:1019: UserWarning:\n",
      "\n",
      "Detected multiclass problem. Forcing interactions to 0. Multiclass interactions only have local explanations. They are not currently displayed in the global explanation visualizations. Set interactions=0 to disable this warning. If you still want multiclass interactions, this API accepts a list, and the measure_interactions function can be used to detect them.\n",
      "\n",
      "/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_40364/3816053816.py:44: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 is out of bounds for axis 0 with size 0\n",
      "index 0 is out of bounds for axis 0 with size 0\n"
     ]
    }
   ],
   "source": [
    "run_report_script_for_dataset(\n",
    "    GENERATED_REPORTS_DIR=GENERATED_REPORTS_DIR,\n",
    "    dataset_id=5,\n",
    "    result_path=\"./data/results/*\",\n",
    "    template_dir=\"./src/website_assets/templates/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automlb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
